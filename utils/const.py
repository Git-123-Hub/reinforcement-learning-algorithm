############################################
# @Author: Git-123-Hub
# @Date: 2021/9/1
# @Description: some constants used in the program
############################################

import matplotlib.pyplot as plt
from torch import nn


class Color:
    """color used for printing and plotting"""
    # color for print to console
    END = '\033[0m'
    SUCCESS = '\033[94m'
    FAIL = '\033[91m'
    INFO = '\033[95m'
    WARNING = '\033[93m'

    cmap = plt.get_cmap('tab10')
    # cmap is a function which can take a value from 0 to 1 and map it to RGBA color

    # color for different agent
    DQN = cmap(0 / 10)
    DDQN = cmap(1 / 10)
    DDQN_PER = cmap(2 / 10)
    REINFORCE = cmap(3 / 10)
    REINFORCE_BASELINE = cmap(4 / 10)
    DDPG = cmap(5 / 10)
    TD3 = cmap(6 / 10)
    SAC = cmap(7 / 10)
    PPO = cmap(8 / 10)
    A3C = cmap(9 / 10)

    # color for different line
    REWARD = cmap(0 / 10)
    TEST = cmap(0 / 10)
    GOAL = cmap(3 / 10)
    RUNNING_REWARD = cmap(1 / 10)


def visualize_color():
    """visualize all the color in COLOR"""
    x = range(1, 11)
    fig, ax = plt.subplots()
    colors = ['DQN', 'DDQN', 'DDQN_PER', 'REINFORCE', 'REINFORCE_BASELINE',
              'DDPG', 'TD3', 'GOAL', 'REWARD', 'TEST', 'RUNNING_REWARD']
    for index, color in enumerate(colors):
        print(getattr(Color, color))
        ax.plot(x, [index] * 10, color=getattr(Color, color), label=color)
    ax.legend()
    plt.show()


class Config:
    """data structure that stores all the hyper-parameters"""

    # NOTE that using an ordinary dict works the same, it's just this could provide code completion and more flexibility
    def __init__(self):
        """list all the parameters that might be used and provide an initial value"""
        # ##### base config #####
        self.seed = None  # global random seed

        # data, graph and policy from each training should be saved
        # so it's necessary to determine the path to store them and whether clear all the old results before start
        self.result_path = './results'  # path to store the graph and data from the training
        # todo: the following two attribute should be removed
        self.results = './results'
        self.policy = './policy'

        # render mode: determine whether render or not,
        # 'train' for rendering during training, 'test' for rendering during testing policy,
        # 'both' for render in both scenario, default 'None', do not render
        self.render = None

        # ##### basic parameter ##### #
        # determine how many episodes should the agent run during each training time
        self.episode_num = 1000
        # if train for multiple run, i.e. use Trainer, run time should also be configured
        self.run_num = 5

        self.gamma = 0.99  # reward discount factor
        self.learning_rate = 1e-3
        self.tau = 0.01  # parameter for network soft-update
        self.update_interval = 5  # interval of updating target_network, if not specified, update every step, i.e. 1

        # ##### replay memory setting ##### #
        self.memory_capacity = 20000
        self.batch_size = 256  # parameters for replay memory
        self.alpha = 0.3
        self.beta = 0.3  # parameters for prioritized replay memory

        self.random_steps = 1000  # interact with the env randomly to generate experience before start to learn

        # ##### epsilon setting ##### #
        self.epsilon = 1  # start epsilon
        self.epsilon_decay_rate = 0.99
        self.min_epsilon = 0.01

        # ##### network setting ##### #
        # hidden layer
        self.q_hidden_layer = [128, 128]
        self.policy_hidden_layer = [128, 128]
        self.actor_hidden_layer = [128, 128]
        self.critic_hidden_layer = [128, 128]
        # activation
        self.q_activation = nn.ReLU()
        self.policy_activation = nn.ReLU()
        self.actor_activation = nn.ReLU()
        self.critic_activation = nn.ReLU()
        self.clip_grad = None
        self.fix_std = None

        # ##### TD3 ##### #
        # todo: explanation of the following parameters
        # in TD3, we generate noise form normal distribution, so mean and std are needed
        # but the noise should be clipped: noise_clip
        # generally we bound it inside min and max action of the env,
        # so noise_clip should multiplied by max_action of the env
        # after clip, we should also decide how much of the clipped noise should be added to the action: noise_factor
        self.noise_mean = 0
        # note that the value below will be multiplies with `max_action` of the env
        self.noise_std = 1
        self.noise_clip = 1
        self.noise_factor = 0.2  # const to scale the noise generated by normal distribution

        # ##### SAC ##### #
        # todo: explanation
        self.entropy_coefficient = None  # default None, i.e. use alpha auto tuning, or you can specify a number

        # ##### parameters for PPO ##### #
        self.training_epoch = 50
        self.clip_ratio = 0.2

        # ##### parameters for A3C ##### #
        self.process_num = None  # number of all the worker, default None, i.e. use all the cpu that's available
        # todo: should learn interval and update interval be distinguished
        self.learn_interval = 5

    # NOTE that method `getitem` and `setitem` are used so that this data structure can be consistent with dict
    def __getitem__(self, key):
        return getattr(self, key)

    def __setitem__(self, key, value):
        setattr(self, key, value)


# default goal for some env whose goal is None
DefaultGoal = {
    'Pendulum-v0': -165,
    'Humanoid‐v3': None,
    'HumanoidStandup‐v2': None,
    'Walker2d‐v3': None,
}

if __name__ == '__main__':
    visualize_color()
